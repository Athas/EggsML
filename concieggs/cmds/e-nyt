#!/usr/bin/env python3
#
# Søg på Hacker News blandt aktuelle historier eller blandt alle historie.
# Anvendelse: e-nyt <søgning> <a[ktuelle] [alle]|g[lobal]>

from lxml.html import fromstring
import json
import random
import re
import requests
import subprocess
import sys
import urllib
import urllib.request as ur

make_danish = False
max_number_of_articles = 10

formatted_links = []
http_error = ""
score_regex = re.compile(r"(\d+)\spoints?")

valid_2nd_args = ["a", "aktuel", "aktuelle", "g", "global"]
if len(sys.argv) < 3 or sys.argv[2] not in valid_2nd_args:
    print("brug: e-nyt <søgning> <a|g>, a => aktuelle historier, g => global søgning i hele historikken")
    exit(0)

if len(sys.argv) > 2 and (sys.argv[2] == "g" or sys.argv[2] == "global"):
    resp = requests.get("https://hn.algolia.com/api/v1/search_by_date?query={}&tags=story&hitsPerPage=1".format(sys.argv[1]))
    data = resp.json()
    output = ""
    num_hits = data["nbHits"]
    if num_hits == 0:
        output = "Jeg kunne ikke finde nogen historier om {} på Hacker News".format(sys.argv[1])
    else:
        story = data["hits"][0]
        number_desc = ""
        if num_hits == 1:
            number_desc = "én historie"
        else:
            number_desc = "{} historier".format(num_hits)

        output = "Hacker News har {} om {}. Den nyeste er \"{}\" på {} med {} point.".format(number_desc, sys.argv[1], story["title"], story["url"], story["points"])

    print(output)
    exit(0)

for i in range(1, 25):
    try:
        f = ur.urlopen("https://news.ycombinator.com/news?p={}".format(str(i)))
        body = f.read()
        document = fromstring(body.decode("utf-8"))
    except urllib.error.HTTPError:

        # Print error to stderr
        import traceback
        traceback.print_exc()

        if (i == 1):
            print("Jeg må slet ikke kravle rundt på Hacker News :(")
            exit(0)
        http_error = "Jeg fik kun lov til at hente de første {} sider. ".format(str(i))
        break;

    stories_and_score =  zip(document.cssselect('tr.athing'), document.cssselect('span.score'))
    for (story, score) in stories_and_score:
        anchor = story.cssselect('td.title > a.storylink')[0]
        text = anchor.text
        link = anchor.get('href')
        score_capture = score_regex.search(score.text)
        formatted_links.append((text, link, int(score_capture.group(1)), i))

extra_output = ""
if (len(sys.argv) > 1):
    ss = sys.argv[1]
    regex = re.compile(r"{}[\W\s$]".format(ss))
    formatted_links = list(filter(lambda x: regex.search(x[0]), formatted_links))
    if len(formatted_links) == 0:
        ss = ss.lower()
        formatted_links = list(filter(lambda x: ss in x[0].lower() or ss in x[1].lower(), formatted_links))
        if len(formatted_links) == 0:
            print("Jeg kunne desværre ikke finde en historie om {}.".format(ss))
            exit(0)

    if len(formatted_links) == 1:
        extra_output = "Hacker News har én aktuel historie om {} :".format(ss)
    else:
        extra_output = "Hacker News har {} aktuelle historier om {}. ".format(str(len(formatted_links)), ss)

    if (len(sys.argv) > 3 and sys.argv[3] == "alle"):
        article_links = ""
        if len(formatted_links) > max_number_of_articles:
            article_links = "De første {} artikler er: ".format(str(max_number_of_articles)) + ", ".join(map(lambda x: x[1], formatted_links[:max_number_of_articles]))
        else:
            article_links = ", ".join(map(lambda x: x[1], formatted_links))

        # No need to make this Danish since it already is
        print(http_error + extra_output + article_links + ".")
        exit(0)

story = random.choice(formatted_links)
if (make_danish):
    process = subprocess.run(['translate', 'en', 'da'], stdout=subprocess.PIPE, input=text.encode('utf-8'))
    text = process.stdout.decode('utf-8').rstrip()

print('{}{}Du kan læse historien "{}" på {}. Den har {} point og blev fundet på side {}.'.format(http_error, extra_output, story[0], story[1], story[2], story[3]))
